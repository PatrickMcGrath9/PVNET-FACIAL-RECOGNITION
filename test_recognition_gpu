import face_recognition
import cv2
import numpy as np
import pickle
import pyttsx3
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# Parameters
frame_skip = 3
frame_count = 0
scaling_factor = 1  # Scale down for faster processing

# Load known faces and encodings
with open("encodings.pkl", "rb") as f:
    known_face_encodings, known_face_names, _ = pickle.load(f)

# Initialize TTS engine
engine = pyttsx3.init()
last_spoken_time = {}
face_detection_time = {}
SPEAK_INTERVAL = 3600  # 1 hour
DETECTION_TIME_REQUIRED = 0.5

# Initialize webcam
video_capture = cv2.VideoCapture(0)
video_capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
video_capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

def speak_name(name):
    """Threaded function to speak a person's name."""
    engine.say(f"Hello {name}")
    engine.runAndWait()

def detect_faces(frame):
    """Detect faces in a frame."""
    return face_recognition.face_locations(frame, model="hog")

def encode_face(frame, face_location):
    """Encode a single face."""
    return face_recognition.face_encodings(frame, [face_location])[0]

# Thread pool with a dynamic number of workers
executor = ThreadPoolExecutor()

while True:
    ret, frame = video_capture.read()
    if not ret:
        break

    frame_count += 1
    if frame_count % frame_skip != 0:
        continue

    start_time = time.time()

    # Resize frame early for faster processing
    small_frame = cv2.resize(frame, (0, 0), fx=scaling_factor, fy=scaling_factor)
    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)

    # Submit detection task
    detection_future = executor.submit(detect_faces, rgb_small_frame)
    face_locations = detection_future.result()

    face_encodings = []
    face_names = []

    if face_locations:
        # Dynamically limit the number of workers based on detected faces
        encoding_executor = ThreadPoolExecutor(max_workers=min(10, len(face_locations)))

        # Encode faces in parallel
        encoding_futures = {encoding_executor.submit(encode_face, rgb_small_frame, loc): loc for loc in face_locations}

        for future in as_completed(encoding_futures):
            face_encoding = future.result()
            face_encodings.append(face_encoding)

        encoding_executor.shutdown(wait=True)

        # Identify faces
        for face_encoding in face_encodings:
            matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.5)
            name = "Unknown"

            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)
            best_match_index = np.argmin(face_distances) if matches else None
            if best_match_index is not None and matches[best_match_index]:
                name = known_face_names[best_match_index]

            # Track and speak names
            current_time = time.time()
            if name not in face_detection_time:
                face_detection_time[name] = current_time

            if current_time - face_detection_time[name] >= DETECTION_TIME_REQUIRED:
                if name not in last_spoken_time or (current_time - last_spoken_time[name]) > SPEAK_INTERVAL:
                    threading.Thread(target=speak_name, args=(name,)).start()
                    last_spoken_time[name] = current_time

            face_names.append(name)

        # Remove old entries
        for name in list(face_detection_time.keys()):
            if name not in face_names:
                del face_detection_time[name]

    # Display the results
    for (top, right, bottom, left), name in zip(face_locations, face_names):
        top = int(top / scaling_factor)
        right = int(right / scaling_factor)
        bottom = int(bottom / scaling_factor)
        left = int(left / scaling_factor)

        # Draw the bounding box and name
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        font_scale = (bottom - top) / 150
        cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, font_scale, (255, 255, 255), 1)

    cv2.imshow('Video', frame)
    print(f"Processing time: {time.time() - start_time:.4f} seconds")

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Clean up
video_capture.release()
cv2.destroyAllWindows()
executor.shutdown()
